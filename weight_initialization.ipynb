{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weight_initialization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiensu/Coding-The-Deep-Learning-Revolution/blob/master/weight_initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vTEA4d_rmj3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbNzlk53mj3M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_path = 'E:\\\\MACHINE_LEARNING\\\\CODING_THE_DEEP_LEARNING_REVOLUTION\\\\PRACTICE\\TensorBoard\\\\'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QReb74Cdmj3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DSISNj8nmj3U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(x_data, y_data, batch_size):\n",
        "    idxs = np.random.randint(0, len(y_data), batch_size)\n",
        "    return x_data[idxs,:,:], y_data[idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dAuEFAOmj3X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maybe_create_folder_structure(sub_folders):\n",
        "    for fold in sub_folders:\n",
        "        if not os.path.isdir(base_path + fold):\n",
        "            os.makedirs(base_path + fold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHxLdu8zmj3b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, initialization, activation, num_layers=3, hidden_size=100):\n",
        "        self._init = initialization\n",
        "        self._activation = activation\n",
        "        # number of layers does not include output layer\n",
        "        self._num_layers = num_layers\n",
        "        self._hidden_size = hidden_size\n",
        "        self._model_def()\n",
        "        \n",
        "    def _model_def(self):\n",
        "        # create placeholder for input\n",
        "        self.input_images = tf.placeholder(tf.float32, shape=[None, 28, 28])\n",
        "        # reshape input x - for 28x28 pixels = 784\n",
        "        x_rs = tf.reshape(self.input_images, shape=[-1, 784])\n",
        "        # scale the input data\n",
        "        input = tf.div(x_rs, 255.0)\n",
        "        # create placeholder for label\n",
        "        self.labels = tf.placeholder(tf.int64, shape=[None, 1])\n",
        "        # convert label data to one hot values\n",
        "        y_one_hot = tf.reshape(tf.one_hot(self.labels, 10), shape=[-1, 10])\n",
        "        \n",
        "        # create self._num_layers dense layers as the model\n",
        "        tf.summary.scalar('input_var', self._calculate_variance(input))\n",
        "        for i in range(self._num_layers-1):\n",
        "            input = tf.layers.dense(input, self._hidden_size, kernel_initializer=self._init, activation=self._activation,\n",
        "                                    name='layer{}'.format(i+1))\n",
        "            # get the input to the nodes\n",
        "            mat_mul_in = tf.get_default_graph().get_tensor_by_name('layer{}/MatMul:0'.format(i+1))\n",
        "            # log pre and post activation function histogram\n",
        "            tf.summary.histogram('mat_mul_in_{}'.format(i+1), mat_mul_in)\n",
        "            tf.summary.histogram('fc_out_{}'.format(i+1), input)\n",
        "            # also log the variance of mat mul\n",
        "            tf.summary.scalar('mat_mul_var_{}'.format(i+1), self._calculate_variance(mat_mul_in))\n",
        "            \n",
        "        # create output layer. Do not supply an activation for the output layer. The loss function definition will supply\n",
        "        # softmax activation. This defaults to a linear activation i.e. f(x) = x\n",
        "        logits = tf.layers.dense(input, 10, name='layer{}'.format(self._num_layers))\n",
        "        mat_mul_in = tf.get_default_graph().get_tensor_by_name('layer{}/MatMul:0'.format(self._num_layers))\n",
        "        tf.summary.histogram('mat_mul_hist_{}'.format(self._num_layers), mat_mul_in)\n",
        "        tf.summary.histogram('fc_out_{}'.format(self._num_layers), input)\n",
        "        \n",
        "        # define loss function, use softmax cross entropy with logits - no need to apply softmax activation to logits\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_one_hot))\n",
        "        # define optimizer function\n",
        "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
        "        # define accuracy function\n",
        "        self.accuracy = self._compute_accuracy(logits, y_one_hot)\n",
        "        \n",
        "        # add the loss, accuracy to the summary\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        tf.summary.scalar('acc', self.accuracy)\n",
        "        self.merged = tf.summary.merge_all()\n",
        "        \n",
        "        # define init variables function\n",
        "        self.init_op = tf.global_variables_initializer()\n",
        "        \n",
        "    def _compute_accuracy(self, logits, labels):\n",
        "        prediction = tf.argmax(logits, 1)\n",
        "        equality = tf.equal(prediction, tf.argmax(labels, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "        return accuracy\n",
        "    \n",
        "    def _calculate_variance(self, x):\n",
        "        mean = tf.reduce_mean(x)\n",
        "        sqr = tf.square(x-mean)\n",
        "        return tf.reduce_mean(sqr)            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atWf5Odrmj3d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_pass_through(model, fold):\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.init_op)\n",
        "        train_writer = tf.summary.FileWriter(base_path+fold, sess.graph)\n",
        "        image_batch, label_batch = get_batch(x_train, y_train, 100)\n",
        "        summary = sess.run(model.merged, feed_dict={model.input_images: image_batch, model.labels: label_batch.reshape(-1, 1)})\n",
        "        train_writer.add_summary(summary, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9s0A6wImj3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, fold, batch_size, epochs):\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.init_op)\n",
        "        train_writer = tf.summary.FileWriter(base_path+fold, sess.graph)\n",
        "        # total_batch = int(len(x_train)/batch_size)\n",
        "        for i in range(epochs):\n",
        "            image_batch, label_batch = get_batch(x_train, y_train, batch_size)\n",
        "            loss, _, acc = sess.run([model.loss, model. optimizer, model.accuracy], \n",
        "                                       feed_dict={model.input_images: image_batch, model.labels: label_batch.reshape(-1,1)})\n",
        "            if i%50==0:\n",
        "                print('Iteration {} of {} - loss: {:.3f}, training accuracy: {:.2f}%'.format(i, epochs, loss, acc*100))\n",
        "                summary = sess.run(model.merged, feed_dict={model.input_images: image_batch, model.labels: label_batch.reshape(-1, 1)})\n",
        "                train_writer.add_summary(summary, i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKC5L2N9mj3i",
        "colab_type": "code",
        "colab": {},
        "outputId": "d85dad1d-41e7-4aae-bee7-b2bb69c75d31"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    sub_folders= ['first_pass_normal', 'first_pass_variance',\n",
        "                 'full_train_normal', 'full_train_variance',\n",
        "                 'full_train_normal_relu', 'full_train_variance_relu',\n",
        "                 'full_train_he_relu']\n",
        "    initializers = [tf.random_normal_initializer,\n",
        "                   tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=False),\n",
        "                   tf.random_normal_initializer,\n",
        "                   tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=False),\n",
        "                   tf.random_normal_initializer,\n",
        "                   tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=False),\n",
        "                   tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)]\n",
        "    activations = [tf.sigmoid, tf.sigmoid, tf.sigmoid, tf.sigmoid, tf.nn.relu, tf.nn.relu, tf.nn.relu]\n",
        "    \n",
        "    assert len(sub_folders) == len(initializers) == len(activations)\n",
        "    \n",
        "    maybe_create_folder_structure(sub_folders)\n",
        "    \n",
        "    for i in range(len(sub_folders)):\n",
        "        tf.reset_default_graph()\n",
        "        model = Model(initializers[i], activations[i])\n",
        "        if 'first_pass' in sub_folders[i]:\n",
        "            init_pass_through(model, sub_folders[i])\n",
        "        else:\n",
        "            train_model(model, sub_folders[i], 32, 5000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 of 5000 - loss: 2.557, training accuracy: 12.50%\n",
            "Iteration 50 of 5000 - loss: 1.960, training accuracy: 37.50%\n",
            "Iteration 100 of 5000 - loss: 1.732, training accuracy: 43.75%\n",
            "Iteration 150 of 5000 - loss: 1.265, training accuracy: 81.25%\n",
            "Iteration 200 of 5000 - loss: 1.415, training accuracy: 53.12%\n",
            "Iteration 250 of 5000 - loss: 1.000, training accuracy: 71.88%\n",
            "Iteration 300 of 5000 - loss: 1.078, training accuracy: 71.88%\n",
            "Iteration 350 of 5000 - loss: 0.766, training accuracy: 81.25%\n",
            "Iteration 400 of 5000 - loss: 0.760, training accuracy: 78.12%\n",
            "Iteration 450 of 5000 - loss: 0.549, training accuracy: 87.50%\n",
            "Iteration 500 of 5000 - loss: 0.843, training accuracy: 71.88%\n",
            "Iteration 550 of 5000 - loss: 0.545, training accuracy: 84.38%\n",
            "Iteration 600 of 5000 - loss: 0.584, training accuracy: 84.38%\n",
            "Iteration 650 of 5000 - loss: 0.355, training accuracy: 93.75%\n",
            "Iteration 700 of 5000 - loss: 0.675, training accuracy: 78.12%\n",
            "Iteration 750 of 5000 - loss: 0.649, training accuracy: 84.38%\n",
            "Iteration 800 of 5000 - loss: 0.784, training accuracy: 81.25%\n",
            "Iteration 850 of 5000 - loss: 0.575, training accuracy: 84.38%\n",
            "Iteration 900 of 5000 - loss: 0.333, training accuracy: 90.62%\n",
            "Iteration 950 of 5000 - loss: 0.239, training accuracy: 90.62%\n",
            "Iteration 1000 of 5000 - loss: 0.430, training accuracy: 90.62%\n",
            "Iteration 1050 of 5000 - loss: 0.194, training accuracy: 96.88%\n",
            "Iteration 1100 of 5000 - loss: 0.391, training accuracy: 96.88%\n",
            "Iteration 1150 of 5000 - loss: 0.380, training accuracy: 87.50%\n",
            "Iteration 1200 of 5000 - loss: 0.418, training accuracy: 90.62%\n",
            "Iteration 1250 of 5000 - loss: 0.374, training accuracy: 93.75%\n",
            "Iteration 1300 of 5000 - loss: 0.256, training accuracy: 96.88%\n",
            "Iteration 1350 of 5000 - loss: 0.338, training accuracy: 90.62%\n",
            "Iteration 1400 of 5000 - loss: 0.417, training accuracy: 84.38%\n",
            "Iteration 1450 of 5000 - loss: 0.278, training accuracy: 87.50%\n",
            "Iteration 1500 of 5000 - loss: 0.204, training accuracy: 96.88%\n",
            "Iteration 1550 of 5000 - loss: 0.418, training accuracy: 90.62%\n",
            "Iteration 1600 of 5000 - loss: 0.495, training accuracy: 90.62%\n",
            "Iteration 1650 of 5000 - loss: 0.296, training accuracy: 93.75%\n",
            "Iteration 1700 of 5000 - loss: 0.227, training accuracy: 93.75%\n",
            "Iteration 1750 of 5000 - loss: 0.113, training accuracy: 96.88%\n",
            "Iteration 1800 of 5000 - loss: 0.459, training accuracy: 81.25%\n",
            "Iteration 1850 of 5000 - loss: 0.512, training accuracy: 84.38%\n",
            "Iteration 1900 of 5000 - loss: 0.237, training accuracy: 90.62%\n",
            "Iteration 1950 of 5000 - loss: 0.260, training accuracy: 93.75%\n",
            "Iteration 2000 of 5000 - loss: 0.253, training accuracy: 90.62%\n",
            "Iteration 2050 of 5000 - loss: 0.153, training accuracy: 100.00%\n",
            "Iteration 2100 of 5000 - loss: 0.232, training accuracy: 93.75%\n",
            "Iteration 2150 of 5000 - loss: 0.185, training accuracy: 100.00%\n",
            "Iteration 2200 of 5000 - loss: 0.435, training accuracy: 87.50%\n",
            "Iteration 2250 of 5000 - loss: 0.339, training accuracy: 87.50%\n",
            "Iteration 2300 of 5000 - loss: 0.400, training accuracy: 87.50%\n",
            "Iteration 2350 of 5000 - loss: 0.229, training accuracy: 87.50%\n",
            "Iteration 2400 of 5000 - loss: 0.270, training accuracy: 87.50%\n",
            "Iteration 2450 of 5000 - loss: 0.126, training accuracy: 96.88%\n",
            "Iteration 2500 of 5000 - loss: 0.243, training accuracy: 93.75%\n",
            "Iteration 2550 of 5000 - loss: 0.215, training accuracy: 96.88%\n",
            "Iteration 2600 of 5000 - loss: 0.288, training accuracy: 93.75%\n",
            "Iteration 2650 of 5000 - loss: 0.309, training accuracy: 93.75%\n",
            "Iteration 2700 of 5000 - loss: 0.326, training accuracy: 87.50%\n",
            "Iteration 2750 of 5000 - loss: 0.108, training accuracy: 93.75%\n",
            "Iteration 2800 of 5000 - loss: 0.223, training accuracy: 93.75%\n",
            "Iteration 2850 of 5000 - loss: 0.223, training accuracy: 93.75%\n",
            "Iteration 2900 of 5000 - loss: 0.376, training accuracy: 84.38%\n",
            "Iteration 2950 of 5000 - loss: 0.296, training accuracy: 87.50%\n",
            "Iteration 3000 of 5000 - loss: 0.221, training accuracy: 90.62%\n",
            "Iteration 3050 of 5000 - loss: 0.074, training accuracy: 100.00%\n",
            "Iteration 3100 of 5000 - loss: 0.289, training accuracy: 90.62%\n",
            "Iteration 3150 of 5000 - loss: 0.250, training accuracy: 93.75%\n",
            "Iteration 3200 of 5000 - loss: 0.248, training accuracy: 93.75%\n",
            "Iteration 3250 of 5000 - loss: 0.342, training accuracy: 84.38%\n",
            "Iteration 3300 of 5000 - loss: 0.255, training accuracy: 90.62%\n",
            "Iteration 3350 of 5000 - loss: 0.384, training accuracy: 84.38%\n",
            "Iteration 3400 of 5000 - loss: 0.358, training accuracy: 90.62%\n",
            "Iteration 3450 of 5000 - loss: 0.321, training accuracy: 93.75%\n",
            "Iteration 3500 of 5000 - loss: 0.200, training accuracy: 90.62%\n",
            "Iteration 3550 of 5000 - loss: 0.161, training accuracy: 93.75%\n",
            "Iteration 3600 of 5000 - loss: 0.267, training accuracy: 90.62%\n",
            "Iteration 3650 of 5000 - loss: 0.311, training accuracy: 87.50%\n",
            "Iteration 3700 of 5000 - loss: 0.031, training accuracy: 100.00%\n",
            "Iteration 3750 of 5000 - loss: 0.086, training accuracy: 96.88%\n",
            "Iteration 3800 of 5000 - loss: 0.118, training accuracy: 100.00%\n",
            "Iteration 3850 of 5000 - loss: 0.092, training accuracy: 96.88%\n",
            "Iteration 3900 of 5000 - loss: 0.226, training accuracy: 93.75%\n",
            "Iteration 3950 of 5000 - loss: 0.114, training accuracy: 96.88%\n",
            "Iteration 4000 of 5000 - loss: 0.146, training accuracy: 96.88%\n",
            "Iteration 4050 of 5000 - loss: 0.257, training accuracy: 90.62%\n",
            "Iteration 4100 of 5000 - loss: 0.229, training accuracy: 93.75%\n",
            "Iteration 4150 of 5000 - loss: 0.231, training accuracy: 93.75%\n",
            "Iteration 4200 of 5000 - loss: 0.344, training accuracy: 87.50%\n",
            "Iteration 4250 of 5000 - loss: 0.240, training accuracy: 93.75%\n",
            "Iteration 4300 of 5000 - loss: 0.102, training accuracy: 96.88%\n",
            "Iteration 4350 of 5000 - loss: 0.067, training accuracy: 100.00%\n",
            "Iteration 4400 of 5000 - loss: 0.175, training accuracy: 93.75%\n",
            "Iteration 4450 of 5000 - loss: 0.119, training accuracy: 100.00%\n",
            "Iteration 4500 of 5000 - loss: 0.209, training accuracy: 96.88%\n",
            "Iteration 4550 of 5000 - loss: 0.041, training accuracy: 100.00%\n",
            "Iteration 4600 of 5000 - loss: 0.115, training accuracy: 96.88%\n",
            "Iteration 4650 of 5000 - loss: 0.175, training accuracy: 96.88%\n",
            "Iteration 4700 of 5000 - loss: 0.445, training accuracy: 84.38%\n",
            "Iteration 4750 of 5000 - loss: 0.051, training accuracy: 100.00%\n",
            "Iteration 4800 of 5000 - loss: 0.152, training accuracy: 90.62%\n",
            "Iteration 4850 of 5000 - loss: 0.226, training accuracy: 93.75%\n",
            "Iteration 4900 of 5000 - loss: 0.046, training accuracy: 100.00%\n",
            "Iteration 4950 of 5000 - loss: 0.238, training accuracy: 87.50%\n",
            "Iteration 0 of 5000 - loss: 2.526, training accuracy: 12.50%\n",
            "Iteration 50 of 5000 - loss: 1.970, training accuracy: 62.50%\n",
            "Iteration 100 of 5000 - loss: 1.617, training accuracy: 68.75%\n",
            "Iteration 150 of 5000 - loss: 1.030, training accuracy: 71.88%\n",
            "Iteration 200 of 5000 - loss: 0.760, training accuracy: 75.00%\n",
            "Iteration 250 of 5000 - loss: 0.995, training accuracy: 81.25%\n",
            "Iteration 300 of 5000 - loss: 0.544, training accuracy: 87.50%\n",
            "Iteration 350 of 5000 - loss: 0.527, training accuracy: 84.38%\n",
            "Iteration 400 of 5000 - loss: 0.536, training accuracy: 84.38%\n",
            "Iteration 450 of 5000 - loss: 0.420, training accuracy: 93.75%\n",
            "Iteration 500 of 5000 - loss: 0.341, training accuracy: 93.75%\n",
            "Iteration 550 of 5000 - loss: 0.281, training accuracy: 93.75%\n",
            "Iteration 600 of 5000 - loss: 0.618, training accuracy: 84.38%\n",
            "Iteration 650 of 5000 - loss: 0.623, training accuracy: 87.50%\n",
            "Iteration 700 of 5000 - loss: 0.177, training accuracy: 96.88%\n",
            "Iteration 750 of 5000 - loss: 0.334, training accuracy: 90.62%\n",
            "Iteration 800 of 5000 - loss: 0.587, training accuracy: 78.12%\n",
            "Iteration 850 of 5000 - loss: 0.111, training accuracy: 100.00%\n",
            "Iteration 900 of 5000 - loss: 0.285, training accuracy: 90.62%\n",
            "Iteration 950 of 5000 - loss: 0.286, training accuracy: 87.50%\n",
            "Iteration 1000 of 5000 - loss: 0.294, training accuracy: 84.38%\n",
            "Iteration 1050 of 5000 - loss: 0.166, training accuracy: 93.75%\n",
            "Iteration 1100 of 5000 - loss: 0.360, training accuracy: 90.62%\n",
            "Iteration 1150 of 5000 - loss: 0.320, training accuracy: 90.62%\n",
            "Iteration 1200 of 5000 - loss: 0.306, training accuracy: 87.50%\n",
            "Iteration 1250 of 5000 - loss: 0.414, training accuracy: 81.25%\n",
            "Iteration 1300 of 5000 - loss: 0.108, training accuracy: 96.88%\n",
            "Iteration 1350 of 5000 - loss: 0.211, training accuracy: 93.75%\n",
            "Iteration 1400 of 5000 - loss: 0.115, training accuracy: 96.88%\n",
            "Iteration 1450 of 5000 - loss: 0.168, training accuracy: 93.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1500 of 5000 - loss: 0.237, training accuracy: 93.75%\n",
            "Iteration 1550 of 5000 - loss: 0.096, training accuracy: 100.00%\n",
            "Iteration 1600 of 5000 - loss: 0.116, training accuracy: 96.88%\n",
            "Iteration 1650 of 5000 - loss: 0.119, training accuracy: 96.88%\n",
            "Iteration 1700 of 5000 - loss: 0.090, training accuracy: 100.00%\n",
            "Iteration 1750 of 5000 - loss: 0.204, training accuracy: 96.88%\n",
            "Iteration 1800 of 5000 - loss: 0.205, training accuracy: 90.62%\n",
            "Iteration 1850 of 5000 - loss: 0.110, training accuracy: 96.88%\n",
            "Iteration 1900 of 5000 - loss: 0.368, training accuracy: 84.38%\n",
            "Iteration 1950 of 5000 - loss: 0.408, training accuracy: 87.50%\n",
            "Iteration 2000 of 5000 - loss: 0.316, training accuracy: 87.50%\n",
            "Iteration 2050 of 5000 - loss: 0.049, training accuracy: 96.88%\n",
            "Iteration 2100 of 5000 - loss: 0.215, training accuracy: 90.62%\n",
            "Iteration 2150 of 5000 - loss: 0.493, training accuracy: 87.50%\n",
            "Iteration 2200 of 5000 - loss: 0.292, training accuracy: 87.50%\n",
            "Iteration 2250 of 5000 - loss: 0.193, training accuracy: 90.62%\n",
            "Iteration 2300 of 5000 - loss: 0.254, training accuracy: 90.62%\n",
            "Iteration 2350 of 5000 - loss: 0.176, training accuracy: 93.75%\n",
            "Iteration 2400 of 5000 - loss: 0.262, training accuracy: 93.75%\n",
            "Iteration 2450 of 5000 - loss: 0.242, training accuracy: 93.75%\n",
            "Iteration 2500 of 5000 - loss: 0.057, training accuracy: 100.00%\n",
            "Iteration 2550 of 5000 - loss: 0.148, training accuracy: 96.88%\n",
            "Iteration 2600 of 5000 - loss: 0.141, training accuracy: 96.88%\n",
            "Iteration 2650 of 5000 - loss: 0.046, training accuracy: 100.00%\n",
            "Iteration 2700 of 5000 - loss: 0.094, training accuracy: 100.00%\n",
            "Iteration 2750 of 5000 - loss: 0.218, training accuracy: 93.75%\n",
            "Iteration 2800 of 5000 - loss: 0.121, training accuracy: 93.75%\n",
            "Iteration 2850 of 5000 - loss: 0.290, training accuracy: 90.62%\n",
            "Iteration 2900 of 5000 - loss: 0.113, training accuracy: 93.75%\n",
            "Iteration 2950 of 5000 - loss: 0.081, training accuracy: 100.00%\n",
            "Iteration 3000 of 5000 - loss: 0.287, training accuracy: 90.62%\n",
            "Iteration 3050 of 5000 - loss: 0.042, training accuracy: 100.00%\n",
            "Iteration 3100 of 5000 - loss: 0.105, training accuracy: 96.88%\n",
            "Iteration 3150 of 5000 - loss: 0.021, training accuracy: 100.00%\n",
            "Iteration 3200 of 5000 - loss: 0.066, training accuracy: 100.00%\n",
            "Iteration 3250 of 5000 - loss: 0.054, training accuracy: 100.00%\n",
            "Iteration 3300 of 5000 - loss: 0.148, training accuracy: 93.75%\n",
            "Iteration 3350 of 5000 - loss: 0.079, training accuracy: 96.88%\n",
            "Iteration 3400 of 5000 - loss: 0.221, training accuracy: 90.62%\n",
            "Iteration 3450 of 5000 - loss: 0.137, training accuracy: 93.75%\n",
            "Iteration 3500 of 5000 - loss: 0.210, training accuracy: 90.62%\n",
            "Iteration 3550 of 5000 - loss: 0.182, training accuracy: 93.75%\n",
            "Iteration 3600 of 5000 - loss: 0.105, training accuracy: 96.88%\n",
            "Iteration 3650 of 5000 - loss: 0.284, training accuracy: 90.62%\n",
            "Iteration 3700 of 5000 - loss: 0.216, training accuracy: 93.75%\n",
            "Iteration 3750 of 5000 - loss: 0.105, training accuracy: 96.88%\n",
            "Iteration 3800 of 5000 - loss: 0.107, training accuracy: 93.75%\n",
            "Iteration 3850 of 5000 - loss: 0.101, training accuracy: 96.88%\n",
            "Iteration 3900 of 5000 - loss: 0.113, training accuracy: 96.88%\n",
            "Iteration 3950 of 5000 - loss: 0.083, training accuracy: 96.88%\n",
            "Iteration 4000 of 5000 - loss: 0.062, training accuracy: 96.88%\n",
            "Iteration 4050 of 5000 - loss: 0.119, training accuracy: 96.88%\n",
            "Iteration 4100 of 5000 - loss: 0.045, training accuracy: 100.00%\n",
            "Iteration 4150 of 5000 - loss: 0.109, training accuracy: 93.75%\n",
            "Iteration 4200 of 5000 - loss: 0.220, training accuracy: 93.75%\n",
            "Iteration 4250 of 5000 - loss: 0.249, training accuracy: 93.75%\n",
            "Iteration 4300 of 5000 - loss: 0.175, training accuracy: 93.75%\n",
            "Iteration 4350 of 5000 - loss: 0.056, training accuracy: 96.88%\n",
            "Iteration 4400 of 5000 - loss: 0.145, training accuracy: 93.75%\n",
            "Iteration 4450 of 5000 - loss: 0.031, training accuracy: 100.00%\n",
            "Iteration 4500 of 5000 - loss: 0.153, training accuracy: 96.88%\n",
            "Iteration 4550 of 5000 - loss: 0.386, training accuracy: 90.62%\n",
            "Iteration 4600 of 5000 - loss: 0.093, training accuracy: 96.88%\n",
            "Iteration 4650 of 5000 - loss: 0.067, training accuracy: 96.88%\n",
            "Iteration 4700 of 5000 - loss: 0.066, training accuracy: 96.88%\n",
            "Iteration 4750 of 5000 - loss: 0.037, training accuracy: 100.00%\n",
            "Iteration 4800 of 5000 - loss: 0.177, training accuracy: 96.88%\n",
            "Iteration 4850 of 5000 - loss: 0.375, training accuracy: 90.62%\n",
            "Iteration 4900 of 5000 - loss: 0.188, training accuracy: 90.62%\n",
            "Iteration 4950 of 5000 - loss: 0.086, training accuracy: 100.00%\n",
            "Iteration 0 of 5000 - loss: 73.016, training accuracy: 6.25%\n",
            "Iteration 50 of 5000 - loss: 17.855, training accuracy: 31.25%\n",
            "Iteration 100 of 5000 - loss: 19.370, training accuracy: 40.62%\n",
            "Iteration 150 of 5000 - loss: 12.662, training accuracy: 56.25%\n",
            "Iteration 200 of 5000 - loss: 12.777, training accuracy: 46.88%\n",
            "Iteration 250 of 5000 - loss: 5.720, training accuracy: 62.50%\n",
            "Iteration 300 of 5000 - loss: 4.501, training accuracy: 75.00%\n",
            "Iteration 350 of 5000 - loss: 3.786, training accuracy: 78.12%\n",
            "Iteration 400 of 5000 - loss: 5.271, training accuracy: 68.75%\n",
            "Iteration 450 of 5000 - loss: 4.601, training accuracy: 71.88%\n",
            "Iteration 500 of 5000 - loss: 2.381, training accuracy: 71.88%\n",
            "Iteration 550 of 5000 - loss: 5.690, training accuracy: 62.50%\n",
            "Iteration 600 of 5000 - loss: 2.038, training accuracy: 75.00%\n",
            "Iteration 650 of 5000 - loss: 1.971, training accuracy: 81.25%\n",
            "Iteration 700 of 5000 - loss: 2.520, training accuracy: 81.25%\n",
            "Iteration 750 of 5000 - loss: 2.991, training accuracy: 75.00%\n",
            "Iteration 800 of 5000 - loss: 2.006, training accuracy: 84.38%\n",
            "Iteration 850 of 5000 - loss: 1.163, training accuracy: 84.38%\n",
            "Iteration 900 of 5000 - loss: 0.534, training accuracy: 93.75%\n",
            "Iteration 950 of 5000 - loss: 2.957, training accuracy: 78.12%\n",
            "Iteration 1000 of 5000 - loss: 0.521, training accuracy: 93.75%\n",
            "Iteration 1050 of 5000 - loss: 2.408, training accuracy: 71.88%\n",
            "Iteration 1100 of 5000 - loss: 3.230, training accuracy: 78.12%\n",
            "Iteration 1150 of 5000 - loss: 1.007, training accuracy: 84.38%\n",
            "Iteration 1200 of 5000 - loss: 1.788, training accuracy: 84.38%\n",
            "Iteration 1250 of 5000 - loss: 0.527, training accuracy: 90.62%\n",
            "Iteration 1300 of 5000 - loss: 2.078, training accuracy: 87.50%\n",
            "Iteration 1350 of 5000 - loss: 2.232, training accuracy: 84.38%\n",
            "Iteration 1400 of 5000 - loss: 2.182, training accuracy: 81.25%\n",
            "Iteration 1450 of 5000 - loss: 0.970, training accuracy: 90.62%\n",
            "Iteration 1500 of 5000 - loss: 1.360, training accuracy: 78.12%\n",
            "Iteration 1550 of 5000 - loss: 0.662, training accuracy: 84.38%\n",
            "Iteration 1600 of 5000 - loss: 3.524, training accuracy: 78.12%\n",
            "Iteration 1650 of 5000 - loss: 0.951, training accuracy: 84.38%\n",
            "Iteration 1700 of 5000 - loss: 1.074, training accuracy: 90.62%\n",
            "Iteration 1750 of 5000 - loss: 1.314, training accuracy: 87.50%\n",
            "Iteration 1800 of 5000 - loss: 2.110, training accuracy: 81.25%\n",
            "Iteration 1850 of 5000 - loss: 0.326, training accuracy: 93.75%\n",
            "Iteration 1900 of 5000 - loss: 0.059, training accuracy: 96.88%\n",
            "Iteration 1950 of 5000 - loss: 0.693, training accuracy: 84.38%\n",
            "Iteration 2000 of 5000 - loss: 1.765, training accuracy: 84.38%\n",
            "Iteration 2050 of 5000 - loss: 0.901, training accuracy: 87.50%\n",
            "Iteration 2100 of 5000 - loss: 0.531, training accuracy: 87.50%\n",
            "Iteration 2150 of 5000 - loss: 0.927, training accuracy: 87.50%\n",
            "Iteration 2200 of 5000 - loss: 1.466, training accuracy: 78.12%\n",
            "Iteration 2250 of 5000 - loss: 1.610, training accuracy: 81.25%\n",
            "Iteration 2300 of 5000 - loss: 1.126, training accuracy: 90.62%\n",
            "Iteration 2350 of 5000 - loss: 0.798, training accuracy: 90.62%\n",
            "Iteration 2400 of 5000 - loss: 1.043, training accuracy: 90.62%\n",
            "Iteration 2450 of 5000 - loss: 1.884, training accuracy: 78.12%\n",
            "Iteration 2500 of 5000 - loss: 0.244, training accuracy: 90.62%\n",
            "Iteration 2550 of 5000 - loss: 1.552, training accuracy: 90.62%\n",
            "Iteration 2600 of 5000 - loss: 0.350, training accuracy: 90.62%\n",
            "Iteration 2650 of 5000 - loss: 0.741, training accuracy: 93.75%\n",
            "Iteration 2700 of 5000 - loss: 0.745, training accuracy: 90.62%\n",
            "Iteration 2750 of 5000 - loss: 0.556, training accuracy: 96.88%\n",
            "Iteration 2800 of 5000 - loss: 1.198, training accuracy: 84.38%\n",
            "Iteration 2850 of 5000 - loss: 1.565, training accuracy: 87.50%\n",
            "Iteration 2900 of 5000 - loss: 0.308, training accuracy: 93.75%\n",
            "Iteration 2950 of 5000 - loss: 0.876, training accuracy: 90.62%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 3000 of 5000 - loss: 0.477, training accuracy: 87.50%\n",
            "Iteration 3050 of 5000 - loss: 0.423, training accuracy: 75.00%\n",
            "Iteration 3100 of 5000 - loss: 1.140, training accuracy: 81.25%\n",
            "Iteration 3150 of 5000 - loss: 0.692, training accuracy: 90.62%\n",
            "Iteration 3200 of 5000 - loss: 1.234, training accuracy: 93.75%\n",
            "Iteration 3250 of 5000 - loss: 0.220, training accuracy: 90.62%\n",
            "Iteration 3300 of 5000 - loss: 0.723, training accuracy: 81.25%\n",
            "Iteration 3350 of 5000 - loss: 0.127, training accuracy: 96.88%\n",
            "Iteration 3400 of 5000 - loss: 1.146, training accuracy: 75.00%\n",
            "Iteration 3450 of 5000 - loss: 1.431, training accuracy: 87.50%\n",
            "Iteration 3500 of 5000 - loss: 0.270, training accuracy: 90.62%\n",
            "Iteration 3550 of 5000 - loss: 0.781, training accuracy: 87.50%\n",
            "Iteration 3600 of 5000 - loss: 0.869, training accuracy: 93.75%\n",
            "Iteration 3650 of 5000 - loss: 1.385, training accuracy: 78.12%\n",
            "Iteration 3700 of 5000 - loss: 0.580, training accuracy: 87.50%\n",
            "Iteration 3750 of 5000 - loss: 0.307, training accuracy: 93.75%\n",
            "Iteration 3800 of 5000 - loss: 0.147, training accuracy: 90.62%\n",
            "Iteration 3850 of 5000 - loss: 0.269, training accuracy: 93.75%\n",
            "Iteration 3900 of 5000 - loss: 1.816, training accuracy: 78.12%\n",
            "Iteration 3950 of 5000 - loss: 0.262, training accuracy: 93.75%\n",
            "Iteration 4000 of 5000 - loss: 0.280, training accuracy: 93.75%\n",
            "Iteration 4050 of 5000 - loss: 0.636, training accuracy: 87.50%\n",
            "Iteration 4100 of 5000 - loss: 0.229, training accuracy: 93.75%\n",
            "Iteration 4150 of 5000 - loss: 0.722, training accuracy: 90.62%\n",
            "Iteration 4200 of 5000 - loss: 0.170, training accuracy: 90.62%\n",
            "Iteration 4250 of 5000 - loss: 0.369, training accuracy: 90.62%\n",
            "Iteration 4300 of 5000 - loss: 0.664, training accuracy: 87.50%\n",
            "Iteration 4350 of 5000 - loss: 0.469, training accuracy: 90.62%\n",
            "Iteration 4400 of 5000 - loss: 0.216, training accuracy: 93.75%\n",
            "Iteration 4450 of 5000 - loss: 1.284, training accuracy: 78.12%\n",
            "Iteration 4500 of 5000 - loss: 0.196, training accuracy: 90.62%\n",
            "Iteration 4550 of 5000 - loss: 0.356, training accuracy: 93.75%\n",
            "Iteration 4600 of 5000 - loss: 0.110, training accuracy: 93.75%\n",
            "Iteration 4650 of 5000 - loss: 0.604, training accuracy: 90.62%\n",
            "Iteration 4700 of 5000 - loss: 0.310, training accuracy: 96.88%\n",
            "Iteration 4750 of 5000 - loss: 0.311, training accuracy: 90.62%\n",
            "Iteration 4800 of 5000 - loss: 0.192, training accuracy: 90.62%\n",
            "Iteration 4850 of 5000 - loss: 0.585, training accuracy: 93.75%\n",
            "Iteration 4900 of 5000 - loss: 0.514, training accuracy: 87.50%\n",
            "Iteration 4950 of 5000 - loss: 0.195, training accuracy: 93.75%\n",
            "Iteration 0 of 5000 - loss: 2.375, training accuracy: 18.75%\n",
            "Iteration 50 of 5000 - loss: 0.573, training accuracy: 90.62%\n",
            "Iteration 100 of 5000 - loss: 0.364, training accuracy: 90.62%\n",
            "Iteration 150 of 5000 - loss: 0.344, training accuracy: 90.62%\n",
            "Iteration 200 of 5000 - loss: 0.276, training accuracy: 93.75%\n",
            "Iteration 250 of 5000 - loss: 0.333, training accuracy: 90.62%\n",
            "Iteration 300 of 5000 - loss: 0.548, training accuracy: 87.50%\n",
            "Iteration 350 of 5000 - loss: 0.188, training accuracy: 96.88%\n",
            "Iteration 400 of 5000 - loss: 0.097, training accuracy: 100.00%\n",
            "Iteration 450 of 5000 - loss: 0.249, training accuracy: 90.62%\n",
            "Iteration 500 of 5000 - loss: 0.304, training accuracy: 90.62%\n",
            "Iteration 550 of 5000 - loss: 0.095, training accuracy: 96.88%\n",
            "Iteration 600 of 5000 - loss: 0.124, training accuracy: 96.88%\n",
            "Iteration 650 of 5000 - loss: 0.314, training accuracy: 87.50%\n",
            "Iteration 700 of 5000 - loss: 0.047, training accuracy: 100.00%\n",
            "Iteration 750 of 5000 - loss: 0.064, training accuracy: 100.00%\n",
            "Iteration 800 of 5000 - loss: 0.084, training accuracy: 96.88%\n",
            "Iteration 850 of 5000 - loss: 0.081, training accuracy: 96.88%\n",
            "Iteration 900 of 5000 - loss: 0.061, training accuracy: 100.00%\n",
            "Iteration 950 of 5000 - loss: 0.223, training accuracy: 93.75%\n",
            "Iteration 1000 of 5000 - loss: 0.190, training accuracy: 93.75%\n",
            "Iteration 1050 of 5000 - loss: 0.233, training accuracy: 90.62%\n",
            "Iteration 1100 of 5000 - loss: 0.162, training accuracy: 96.88%\n",
            "Iteration 1150 of 5000 - loss: 0.123, training accuracy: 96.88%\n",
            "Iteration 1200 of 5000 - loss: 0.201, training accuracy: 93.75%\n",
            "Iteration 1250 of 5000 - loss: 0.066, training accuracy: 100.00%\n",
            "Iteration 1300 of 5000 - loss: 0.534, training accuracy: 87.50%\n",
            "Iteration 1350 of 5000 - loss: 0.068, training accuracy: 96.88%\n",
            "Iteration 1400 of 5000 - loss: 0.213, training accuracy: 93.75%\n",
            "Iteration 1450 of 5000 - loss: 0.074, training accuracy: 93.75%\n",
            "Iteration 1500 of 5000 - loss: 0.042, training accuracy: 100.00%\n",
            "Iteration 1550 of 5000 - loss: 0.038, training accuracy: 100.00%\n",
            "Iteration 1600 of 5000 - loss: 0.024, training accuracy: 100.00%\n",
            "Iteration 1650 of 5000 - loss: 0.161, training accuracy: 96.88%\n",
            "Iteration 1700 of 5000 - loss: 0.066, training accuracy: 96.88%\n",
            "Iteration 1750 of 5000 - loss: 0.081, training accuracy: 96.88%\n",
            "Iteration 1800 of 5000 - loss: 0.085, training accuracy: 100.00%\n",
            "Iteration 1850 of 5000 - loss: 0.262, training accuracy: 90.62%\n",
            "Iteration 1900 of 5000 - loss: 0.055, training accuracy: 100.00%\n",
            "Iteration 1950 of 5000 - loss: 0.077, training accuracy: 96.88%\n",
            "Iteration 2000 of 5000 - loss: 0.042, training accuracy: 96.88%\n",
            "Iteration 2050 of 5000 - loss: 0.261, training accuracy: 93.75%\n",
            "Iteration 2100 of 5000 - loss: 0.149, training accuracy: 93.75%\n",
            "Iteration 2150 of 5000 - loss: 0.118, training accuracy: 93.75%\n",
            "Iteration 2200 of 5000 - loss: 0.087, training accuracy: 96.88%\n",
            "Iteration 2250 of 5000 - loss: 0.043, training accuracy: 96.88%\n",
            "Iteration 2300 of 5000 - loss: 0.055, training accuracy: 96.88%\n",
            "Iteration 2350 of 5000 - loss: 0.136, training accuracy: 93.75%\n",
            "Iteration 2400 of 5000 - loss: 0.106, training accuracy: 93.75%\n",
            "Iteration 2450 of 5000 - loss: 0.050, training accuracy: 100.00%\n",
            "Iteration 2500 of 5000 - loss: 0.057, training accuracy: 96.88%\n",
            "Iteration 2550 of 5000 - loss: 0.024, training accuracy: 100.00%\n",
            "Iteration 2600 of 5000 - loss: 0.091, training accuracy: 96.88%\n",
            "Iteration 2650 of 5000 - loss: 0.049, training accuracy: 96.88%\n",
            "Iteration 2700 of 5000 - loss: 0.097, training accuracy: 93.75%\n",
            "Iteration 2750 of 5000 - loss: 0.119, training accuracy: 93.75%\n",
            "Iteration 2800 of 5000 - loss: 0.158, training accuracy: 96.88%\n",
            "Iteration 2850 of 5000 - loss: 0.028, training accuracy: 100.00%\n",
            "Iteration 2900 of 5000 - loss: 0.034, training accuracy: 100.00%\n",
            "Iteration 2950 of 5000 - loss: 0.020, training accuracy: 100.00%\n",
            "Iteration 3000 of 5000 - loss: 0.111, training accuracy: 96.88%\n",
            "Iteration 3050 of 5000 - loss: 0.011, training accuracy: 100.00%\n",
            "Iteration 3100 of 5000 - loss: 0.316, training accuracy: 84.38%\n",
            "Iteration 3150 of 5000 - loss: 0.085, training accuracy: 96.88%\n",
            "Iteration 3200 of 5000 - loss: 0.021, training accuracy: 100.00%\n",
            "Iteration 3250 of 5000 - loss: 0.033, training accuracy: 100.00%\n",
            "Iteration 3300 of 5000 - loss: 0.022, training accuracy: 100.00%\n",
            "Iteration 3350 of 5000 - loss: 0.016, training accuracy: 100.00%\n",
            "Iteration 3400 of 5000 - loss: 0.026, training accuracy: 100.00%\n",
            "Iteration 3450 of 5000 - loss: 0.067, training accuracy: 96.88%\n",
            "Iteration 3500 of 5000 - loss: 0.010, training accuracy: 100.00%\n",
            "Iteration 3550 of 5000 - loss: 0.107, training accuracy: 96.88%\n",
            "Iteration 3600 of 5000 - loss: 0.012, training accuracy: 100.00%\n",
            "Iteration 3650 of 5000 - loss: 0.006, training accuracy: 100.00%\n",
            "Iteration 3700 of 5000 - loss: 0.038, training accuracy: 100.00%\n",
            "Iteration 3750 of 5000 - loss: 0.013, training accuracy: 100.00%\n",
            "Iteration 3800 of 5000 - loss: 0.041, training accuracy: 96.88%\n",
            "Iteration 3850 of 5000 - loss: 0.050, training accuracy: 100.00%\n",
            "Iteration 3900 of 5000 - loss: 0.028, training accuracy: 100.00%\n",
            "Iteration 3950 of 5000 - loss: 0.004, training accuracy: 100.00%\n",
            "Iteration 4000 of 5000 - loss: 0.020, training accuracy: 100.00%\n",
            "Iteration 4050 of 5000 - loss: 0.023, training accuracy: 100.00%\n",
            "Iteration 4100 of 5000 - loss: 0.003, training accuracy: 100.00%\n",
            "Iteration 4150 of 5000 - loss: 0.028, training accuracy: 100.00%\n",
            "Iteration 4200 of 5000 - loss: 0.064, training accuracy: 96.88%\n",
            "Iteration 4250 of 5000 - loss: 0.089, training accuracy: 93.75%\n",
            "Iteration 4300 of 5000 - loss: 0.047, training accuracy: 100.00%\n",
            "Iteration 4350 of 5000 - loss: 0.181, training accuracy: 90.62%\n",
            "Iteration 4400 of 5000 - loss: 0.059, training accuracy: 96.88%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 4450 of 5000 - loss: 0.013, training accuracy: 100.00%\n",
            "Iteration 4500 of 5000 - loss: 0.101, training accuracy: 96.88%\n",
            "Iteration 4550 of 5000 - loss: 0.010, training accuracy: 100.00%\n",
            "Iteration 4600 of 5000 - loss: 0.020, training accuracy: 100.00%\n",
            "Iteration 4650 of 5000 - loss: 0.060, training accuracy: 96.88%\n",
            "Iteration 4700 of 5000 - loss: 0.012, training accuracy: 100.00%\n",
            "Iteration 4750 of 5000 - loss: 0.104, training accuracy: 96.88%\n",
            "Iteration 4800 of 5000 - loss: 0.041, training accuracy: 96.88%\n",
            "Iteration 4850 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 4900 of 5000 - loss: 0.022, training accuracy: 100.00%\n",
            "Iteration 4950 of 5000 - loss: 0.031, training accuracy: 100.00%\n",
            "Iteration 0 of 5000 - loss: 2.548, training accuracy: 12.50%\n",
            "Iteration 50 of 5000 - loss: 0.529, training accuracy: 81.25%\n",
            "Iteration 100 of 5000 - loss: 0.118, training accuracy: 100.00%\n",
            "Iteration 150 of 5000 - loss: 0.450, training accuracy: 90.62%\n",
            "Iteration 200 of 5000 - loss: 0.317, training accuracy: 90.62%\n",
            "Iteration 250 of 5000 - loss: 0.170, training accuracy: 93.75%\n",
            "Iteration 300 of 5000 - loss: 0.207, training accuracy: 90.62%\n",
            "Iteration 350 of 5000 - loss: 0.137, training accuracy: 96.88%\n",
            "Iteration 400 of 5000 - loss: 0.352, training accuracy: 87.50%\n",
            "Iteration 450 of 5000 - loss: 0.361, training accuracy: 90.62%\n",
            "Iteration 500 of 5000 - loss: 0.246, training accuracy: 87.50%\n",
            "Iteration 550 of 5000 - loss: 0.109, training accuracy: 96.88%\n",
            "Iteration 600 of 5000 - loss: 0.084, training accuracy: 93.75%\n",
            "Iteration 650 of 5000 - loss: 0.337, training accuracy: 87.50%\n",
            "Iteration 700 of 5000 - loss: 0.165, training accuracy: 93.75%\n",
            "Iteration 750 of 5000 - loss: 0.119, training accuracy: 96.88%\n",
            "Iteration 800 of 5000 - loss: 0.218, training accuracy: 96.88%\n",
            "Iteration 850 of 5000 - loss: 0.165, training accuracy: 93.75%\n",
            "Iteration 900 of 5000 - loss: 0.049, training accuracy: 96.88%\n",
            "Iteration 950 of 5000 - loss: 0.142, training accuracy: 93.75%\n",
            "Iteration 1000 of 5000 - loss: 0.242, training accuracy: 90.62%\n",
            "Iteration 1050 of 5000 - loss: 0.102, training accuracy: 96.88%\n",
            "Iteration 1100 of 5000 - loss: 0.110, training accuracy: 96.88%\n",
            "Iteration 1150 of 5000 - loss: 0.138, training accuracy: 93.75%\n",
            "Iteration 1200 of 5000 - loss: 0.024, training accuracy: 100.00%\n",
            "Iteration 1250 of 5000 - loss: 0.125, training accuracy: 93.75%\n",
            "Iteration 1300 of 5000 - loss: 0.094, training accuracy: 96.88%\n",
            "Iteration 1350 of 5000 - loss: 0.086, training accuracy: 96.88%\n",
            "Iteration 1400 of 5000 - loss: 0.198, training accuracy: 93.75%\n",
            "Iteration 1450 of 5000 - loss: 0.415, training accuracy: 84.38%\n",
            "Iteration 1500 of 5000 - loss: 0.155, training accuracy: 93.75%\n",
            "Iteration 1550 of 5000 - loss: 0.377, training accuracy: 87.50%\n",
            "Iteration 1600 of 5000 - loss: 0.026, training accuracy: 100.00%\n",
            "Iteration 1650 of 5000 - loss: 0.274, training accuracy: 93.75%\n",
            "Iteration 1700 of 5000 - loss: 0.200, training accuracy: 93.75%\n",
            "Iteration 1750 of 5000 - loss: 0.042, training accuracy: 96.88%\n",
            "Iteration 1800 of 5000 - loss: 0.050, training accuracy: 100.00%\n",
            "Iteration 1850 of 5000 - loss: 0.332, training accuracy: 90.62%\n",
            "Iteration 1900 of 5000 - loss: 0.018, training accuracy: 100.00%\n",
            "Iteration 1950 of 5000 - loss: 0.171, training accuracy: 93.75%\n",
            "Iteration 2000 of 5000 - loss: 0.056, training accuracy: 96.88%\n",
            "Iteration 2050 of 5000 - loss: 0.020, training accuracy: 100.00%\n",
            "Iteration 2100 of 5000 - loss: 0.107, training accuracy: 96.88%\n",
            "Iteration 2150 of 5000 - loss: 0.109, training accuracy: 96.88%\n",
            "Iteration 2200 of 5000 - loss: 0.155, training accuracy: 93.75%\n",
            "Iteration 2250 of 5000 - loss: 0.019, training accuracy: 100.00%\n",
            "Iteration 2300 of 5000 - loss: 0.138, training accuracy: 90.62%\n",
            "Iteration 2350 of 5000 - loss: 0.194, training accuracy: 96.88%\n",
            "Iteration 2400 of 5000 - loss: 0.021, training accuracy: 100.00%\n",
            "Iteration 2450 of 5000 - loss: 0.226, training accuracy: 96.88%\n",
            "Iteration 2500 of 5000 - loss: 0.045, training accuracy: 100.00%\n",
            "Iteration 2550 of 5000 - loss: 0.057, training accuracy: 96.88%\n",
            "Iteration 2600 of 5000 - loss: 0.037, training accuracy: 100.00%\n",
            "Iteration 2650 of 5000 - loss: 0.153, training accuracy: 93.75%\n",
            "Iteration 2700 of 5000 - loss: 0.050, training accuracy: 100.00%\n",
            "Iteration 2750 of 5000 - loss: 0.107, training accuracy: 93.75%\n",
            "Iteration 2800 of 5000 - loss: 0.139, training accuracy: 96.88%\n",
            "Iteration 2850 of 5000 - loss: 0.104, training accuracy: 93.75%\n",
            "Iteration 2900 of 5000 - loss: 0.032, training accuracy: 100.00%\n",
            "Iteration 2950 of 5000 - loss: 0.055, training accuracy: 96.88%\n",
            "Iteration 3000 of 5000 - loss: 0.019, training accuracy: 100.00%\n",
            "Iteration 3050 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 3100 of 5000 - loss: 0.015, training accuracy: 100.00%\n",
            "Iteration 3150 of 5000 - loss: 0.031, training accuracy: 100.00%\n",
            "Iteration 3200 of 5000 - loss: 0.139, training accuracy: 93.75%\n",
            "Iteration 3250 of 5000 - loss: 0.079, training accuracy: 96.88%\n",
            "Iteration 3300 of 5000 - loss: 0.065, training accuracy: 96.88%\n",
            "Iteration 3350 of 5000 - loss: 0.143, training accuracy: 96.88%\n",
            "Iteration 3400 of 5000 - loss: 0.021, training accuracy: 100.00%\n",
            "Iteration 3450 of 5000 - loss: 0.059, training accuracy: 96.88%\n",
            "Iteration 3500 of 5000 - loss: 0.053, training accuracy: 100.00%\n",
            "Iteration 3550 of 5000 - loss: 0.303, training accuracy: 93.75%\n",
            "Iteration 3600 of 5000 - loss: 0.007, training accuracy: 100.00%\n",
            "Iteration 3650 of 5000 - loss: 0.017, training accuracy: 100.00%\n",
            "Iteration 3700 of 5000 - loss: 0.089, training accuracy: 96.88%\n",
            "Iteration 3750 of 5000 - loss: 0.013, training accuracy: 100.00%\n",
            "Iteration 3800 of 5000 - loss: 0.016, training accuracy: 100.00%\n",
            "Iteration 3850 of 5000 - loss: 0.066, training accuracy: 96.88%\n",
            "Iteration 3900 of 5000 - loss: 0.101, training accuracy: 96.88%\n",
            "Iteration 3950 of 5000 - loss: 0.013, training accuracy: 100.00%\n",
            "Iteration 4000 of 5000 - loss: 0.047, training accuracy: 96.88%\n",
            "Iteration 4050 of 5000 - loss: 0.068, training accuracy: 96.88%\n",
            "Iteration 4100 of 5000 - loss: 0.032, training accuracy: 100.00%\n",
            "Iteration 4150 of 5000 - loss: 0.014, training accuracy: 100.00%\n",
            "Iteration 4200 of 5000 - loss: 0.252, training accuracy: 93.75%\n",
            "Iteration 4250 of 5000 - loss: 0.168, training accuracy: 93.75%\n",
            "Iteration 4300 of 5000 - loss: 0.107, training accuracy: 96.88%\n",
            "Iteration 4350 of 5000 - loss: 0.044, training accuracy: 96.88%\n",
            "Iteration 4400 of 5000 - loss: 0.031, training accuracy: 100.00%\n",
            "Iteration 4450 of 5000 - loss: 0.036, training accuracy: 100.00%\n",
            "Iteration 4500 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 4550 of 5000 - loss: 0.011, training accuracy: 100.00%\n",
            "Iteration 4600 of 5000 - loss: 0.230, training accuracy: 96.88%\n",
            "Iteration 4650 of 5000 - loss: 0.017, training accuracy: 100.00%\n",
            "Iteration 4700 of 5000 - loss: 0.066, training accuracy: 96.88%\n",
            "Iteration 4750 of 5000 - loss: 0.023, training accuracy: 100.00%\n",
            "Iteration 4800 of 5000 - loss: 0.012, training accuracy: 100.00%\n",
            "Iteration 4850 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 4900 of 5000 - loss: 0.012, training accuracy: 100.00%\n",
            "Iteration 4950 of 5000 - loss: 0.011, training accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}