{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visualization Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiensu/Coding-The-Deep-Learning-Revolution/blob/master/Visualization_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x_qgt2_X69kC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uVgnm6J1mTWm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STORE_PATH = 'E:\\MACHINE_LEARNING\\CODING_THE_DEEP_LEARNING_REVOLUTION\\PRACTICE'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "damJEACq05OA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract the training data in batches of samples\n",
        "def get_batch(x_data, y_data, batch_size):\n",
        "  idxs = np.random.randint(0, len(y_data), batch_size)\n",
        "  return x_data[idxs, :, :], y_data[idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2lilKiSX2aGe"
      },
      "cell_type": "markdown",
      "source": [
        "The x data is the image information – 60,000 images of 28 x 28 pixels size in the training set. The\n",
        "images are grayscale (i.e black and white) with maximum values, specifying the intensity of whites,\n",
        "of 255. The x data will need to be scaled so that it resides between 0 and 1, as this improves training\n",
        "efficiency. The y data is the matching image labels – signifying what digit is displayed in the image.\n",
        "This will need to be transformed to “one hot” format."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ha-GHjwX2cCE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_example():\n",
        "    # load data\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # python optimisation variables\n",
        "    learning_rate = 0.5\n",
        "    epochs = 10\n",
        "    batch_size = 100\n",
        "    \n",
        "    with tf.name_scope(\"inputs\"):\n",
        "        # declare the training data placeholders\n",
        "        x = tf.placeholder(tf.float32, [None, 28, 28])\n",
        "        # reshape input x - for 28 x 28 pixels = 784\n",
        "        x_rs = tf.reshape(x, [-1, 784])\n",
        "        # scale the input data (maximum is 255.0, minimum is 0.0)\n",
        "        x_sc = tf.div(x_rs, 255.0)\n",
        "        # now declare the output data placeholder - 10 digits\n",
        "        y = tf.placeholder(tf.int64, [None, 1])\n",
        "        # convert the y data to one hot values\n",
        "        y_one_hot = tf.reshape(tf.one_hot(y, 10), [-1, 10])\n",
        "        \n",
        "    with tf.name_scope(\"layer_1\"):\n",
        "        # now declare the weights connecting the input to the hidden layer\n",
        "        W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W')\n",
        "        b1 = tf.Variable(tf.random_normal([300]), name='b')\n",
        "        hidden_logits = tf.add(tf.matmul(x_sc, W1), b1)\n",
        "        hidden_out = tf.nn.sigmoid(hidden_logits)\n",
        "        tf.summary.histogram(\"Hidden_logits\", hidden_logits)\n",
        "        tf.summary.histogram(\"Hidden_output\", hidden_out)\n",
        "        \n",
        "    with tf.name_scope(\"layer_2\"):\n",
        "        # and the weights connecting the hidden layer to the output layer\n",
        "        W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.05), name='W')\n",
        "        b2 = tf.Variable(tf.random_normal([10]), name='b')\n",
        "        logits = tf.add(tf.matmul(hidden_out, W2), b2)\n",
        "    \n",
        "    # now let's define the cost function which we are going to train the model on\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_one_hot, logits=logits))\n",
        "    \n",
        "    # add an optimiser\n",
        "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "    \n",
        "    # finally setup the initialisation operator\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    \n",
        "    # define an accuracy assessment operation\n",
        "    with tf.name_scope(\"accuracy\"):\n",
        "        correct_prediction = tf.equal(tf.argmax(y_one_hot, 1), tf.argmax(logits, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        \n",
        "    with tf.variable_scope(\"getimages\"):\n",
        "        correct_inputs = tf.boolean_mask(x_sc, correct_prediction)\n",
        "        image_summary_true = tf.summary.image('correct_images', tf.reshape(correct_inputs, (-1, 28, 28, 1)), max_outputs=5)\n",
        "        incorrect_inputs = tf.boolean_mask(x_sc, tf.logical_not(correct_prediction))\n",
        "        image_summary_false = tf.summary.image('incorrect_images', tf.reshape(incorrect_inputs, (-1, 28, 28, 1)), max_outputs=5)\n",
        "        \n",
        "    # add a summary to store the accuracy\n",
        "    tf.summary.scalar('acc_summary', accuracy)\n",
        "    # merge all\n",
        "    merged = tf.summary.merge_all()\n",
        "    \n",
        "    # start the session\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "        writer = tf.summary.FileWriter(STORE_PATH, sess.graph)\n",
        "        # initialise the variables\n",
        "        total_batch = int(len(y_train) / batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            avg_cost = 0\n",
        "            for i in range(total_batch):\n",
        "                batch_x, batch_y = get_batch(x_train, y_train, batch_size=batch_size)\n",
        "                _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y.reshape(-1, 1)})\n",
        "                avg_cost += c / total_batch\n",
        "            acc, summary = sess.run([accuracy, merged], feed_dict={x: x_test, y: y_test.reshape(-1, 1)})\n",
        "            print(\"Epoch: {}, cost={:.3f}, test set accuracy={:.3f}%\".format(epoch + 1, avg_cost, acc*100))\n",
        "            writer.add_summary(summary, epoch)\n",
        "        print(\"\\nTraining complete!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7H32cDNs6kaa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nn_example()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}