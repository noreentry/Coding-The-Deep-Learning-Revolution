{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanishing_gradient.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiensu/Coding-The-Deep-Learning-Revolution/blob/master/vanishing_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rWvbIsVvlm-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FhcXCjZclm-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_path = 'E:\\MACHINE_LEARNING\\CODING_THE_DEEP_LEARNING_REVOLUTION\\PRACTICE\\TensorBoard'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJG2awbClm-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ey-zbGtflm-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(x_data, y_data, batch_size):\n",
        "    idxs = np.random.randint(0, len(y_data), batch_size)\n",
        "    return x_data[idxs,:,:], y_data[idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VRVcHkpRlm-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, activation, num_layers=6, hidden_size=10):\n",
        "        self._activation = activation\n",
        "        # number of layers does not include the output layer\n",
        "        self._num_layers = num_layers\n",
        "        self._hidden_size = hidden_size\n",
        "        self._model_def()\n",
        "    \n",
        "    def _model_def(self):\n",
        "        # create placeholder for input image variables\n",
        "        self.input_images = tf.placeholder(tf.float32, shape=[None, 28, 28])\n",
        "        # reshape input x - for 28x28 pixels = 784\n",
        "        x_rs = tf.reshape(self.input_images, shape=[-1, 784])\n",
        "        # scale the input data\n",
        "        input = tf.div(x_rs, 255.0)\n",
        "        # create placeholder for labels variables\n",
        "        self.labels = tf.placeholder(tf.int64, shape=[None, 1])\n",
        "        # convert the label data to one hot values\n",
        "        y_one_hot = tf.reshape(tf.one_hot(self.labels, 10), shape=[-1, 10])\n",
        "        \n",
        "        # create self._num_layers dense layers as the model\n",
        "        for i in range(self._num_layers - 1):\n",
        "            input = tf.layers.dense(input, self._hidden_size, activation=self._activation, name='layer{}'.format(i+1))\n",
        "        # create output layer. Do not supply an activation for the final layer. The loss function definition will supply softmax\n",
        "        # activation. This defaults to a linear activation i.e. f(x) = x\n",
        "        logits = tf.layers.dense(input, 10, name='layer{}'.format(self._num_layers))\n",
        "        \n",
        "        # definition loss function. Use softmax cross entropy with logits. No need to apply softmax activation to logits\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_one_hot))\n",
        "        \n",
        "        # add the loss, accuracy to the summary\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        self._log_gradients(self._num_layers)\n",
        "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
        "        self.accuracy = self._compute_accuracy(logits, y_one_hot)\n",
        "        tf.summary.scalar('accuracy', self.accuracy)\n",
        "        self.merged = tf.summary.merge_all()\n",
        "        self.init_op = tf.global_variables_initializer()\n",
        "    \n",
        "    def _compute_accuracy(self, logits, labels):\n",
        "        prediction = tf.argmax(logits, 1)\n",
        "        equality = tf.equal(prediction, tf.argmax(labels, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "        return accuracy\n",
        "    \n",
        "    def _log_gradients(self, num_layers):\n",
        "        gr = tf.get_default_graph()\n",
        "        for i in range(num_layers):\n",
        "            weight = gr.get_tensor_by_name('layer{}/kernel:0'.format(i+1))\n",
        "            grad = tf.gradients(self.loss, weight)[0]\n",
        "            mean = tf.reduce_mean(tf.abs(grad))\n",
        "            tf.summary.scalar('mean_{}'.format(i+1), mean)\n",
        "            tf.summary.histogram('histogram_{}'.format(i+1), grad)\n",
        "            tf.summary.histogram('hist_weight_{}'.format(i+1), grad)            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmuOX5Iolm-q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_training(model, sub_folder, epochs=50, batch_size=32):\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.init_op)\n",
        "        train_writer = tf.summary.FileWriter(base_path + sub_folder, sess.graph)\n",
        "        #total_batch = int(len(y_train)/batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            image_batch, label_batch = get_batch(x_train, y_train, batch_size)\n",
        "            l, _, acc = sess.run([model.loss, model.optimizer, model.accuracy], \n",
        "                            feed_dict={model.input_images: image_batch,\n",
        "                                      model.labels: label_batch.reshape(-1,1)})\n",
        "            if epoch%10 == 0:\n",
        "                summary = sess.run(model.merged, feed_dict={model.input_images: image_batch, model.labels: label_batch.reshape(-1,1)})\n",
        "                train_writer.add_summary(summary, epoch)\n",
        "                print('Iteration {} of {}, loss: {:.3f}, train accuracy: {:.2f}%'.format(epoch, epochs, l, acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Trfepy-ilm-u",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7b71ad3-cf4a-421f-ebe8-7c30619368a2"
      },
      "cell_type": "code",
      "source": [
        "# main\n",
        "if __name__ == \"__main__\":\n",
        "    scenarios = ['sigmoid', 'relu', 'leaky relu']\n",
        "    act_funcs = [tf.sigmoid, tf.nn.relu, tf.nn.leaky_relu]\n",
        "    assert len(scenarios) == len(act_funcs)\n",
        "    \n",
        "    # collect the training data\n",
        "    for i in range(len(scenarios)):\n",
        "        tf.reset_default_graph()\n",
        "        print('Running scenarios: {}'.format(scenarios[i]))\n",
        "        model = Model(act_funcs[i], 6, 10)\n",
        "        run_training(model, scenarios[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running scenarios: sigmoid\n",
            "Iteration 0 of 50, loss: 1.308, train accuracy: 53.12%\n",
            "Iteration 10 of 50, loss: 0.635, train accuracy: 84.38%\n",
            "Iteration 20 of 50, loss: 0.248, train accuracy: 93.75%\n",
            "Iteration 30 of 50, loss: 0.656, train accuracy: 84.38%\n",
            "Iteration 40 of 50, loss: 0.300, train accuracy: 93.75%\n",
            "Running scenarios: relu\n",
            "Iteration 0 of 50, loss: 0.171, train accuracy: 96.88%\n",
            "Iteration 10 of 50, loss: 0.138, train accuracy: 93.75%\n",
            "Iteration 20 of 50, loss: 0.199, train accuracy: 93.75%\n",
            "Iteration 30 of 50, loss: 0.091, train accuracy: 100.00%\n",
            "Iteration 40 of 50, loss: 0.108, train accuracy: 93.75%\n",
            "Running scenarios: leaky relu\n",
            "Iteration 0 of 50, loss: 0.414, train accuracy: 90.62%\n",
            "Iteration 10 of 50, loss: 0.080, train accuracy: 100.00%\n",
            "Iteration 20 of 50, loss: 0.185, train accuracy: 96.88%\n",
            "Iteration 30 of 50, loss: 0.254, train accuracy: 87.50%\n",
            "Iteration 40 of 50, loss: 0.097, train accuracy: 96.88%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}