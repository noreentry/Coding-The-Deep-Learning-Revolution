{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiensu/Coding-The-Deep-Learning-Revolution/blob/master/mnist_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "25kvkGBRlWcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime as dt\n",
        "import os\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7CKnFg8lWcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRdgVrk4lWcP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_path = 'D:\\\\MACHINE_LEARNING\\\\EBOOKs\\\\CODING_THE_DEEP_LEARNING_REVOLUTION\\\\PRACTICE\\\\Tensorboard\\\\'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B2Knl4xIlWcS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(x_data, y_data, batch_size):\n",
        "    idxs = np.random.randint(0, len(y_data), batch_size)\n",
        "    return x_data[idxs,:,:], y_data[idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKbmBU4VlWcU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(object):\n",
        "    def __init__(self):\n",
        "        self._model_def()\n",
        "        \n",
        "    def _model_def(self):\n",
        "        # create placeholder for input variables\n",
        "        self.input_images = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "        # scale input data\n",
        "        input = tf.div(self.input_images, 255.0)\n",
        "        # create place holder for label\n",
        "        self.labels = tf.placeholder(tf.int64, shape=[None, 1])\n",
        "        # convert the label to one hot value\n",
        "        y_one_hot = tf.reshape(tf.one_hot(self.labels, 10), [-1, 10])\n",
        "        \n",
        "        # create some convolutional layers\n",
        "        layer1 = self._create_new_conv_layer(input, 1, 32, [5, 5], [2, 2], [1, 1], [2, 2], name='layer1')\n",
        "        layer2 = self._create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], [1, 1], [2, 2], name='layer2')\n",
        "        \n",
        "        # flatten the output ready for the fully connected output stage - after two layers of stride 2 pooling, we go\n",
        "        # from 28x28, to 14x14 to 7x7 (64 channels). To create the fully connected, \"dense\" layer, the new shape need \n",
        "        # to be [-1, 7*7*64]. However,bellow is a generalized method of determining the flattened shape.\n",
        "        shape = layer2.get_shape().as_list()\n",
        "        flattened_size = shape[1]*shape[2]*shape[3]\n",
        "        flattened = tf.reshape(layer2, shape=[-1, flattened_size])\n",
        "        \n",
        "        initializer = tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=False)\n",
        "        self.prob = tf.placeholder_with_default(1.0, shape=())\n",
        "        with tf.name_scope(\"first_dense\"):\n",
        "            # setup some weights and bias values for this layer, then activate with ReLU\n",
        "            wd1 = tf.Variable(initializer((flattened_size, 300)), name='wd1')\n",
        "            bd1 = tf.Variable(tf.zeros(300), name='bd1')\n",
        "            dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
        "            dense_layer1 = tf.nn.relu(dense_layer1)\n",
        "            dense_layer1 = tf.nn.dropout(dense_layer1, self.prob)\n",
        "        with tf.name_scope(\"second_dense\"):\n",
        "            # another layer with softmax activations\n",
        "            wd2 = tf.Variable(initializer((300, 10)), name='wd2')\n",
        "            bd2 = tf.Variable(tf.zeros(10), name='bd2')\n",
        "            logits = tf.matmul(dense_layer1, wd2) + bd2\n",
        "            \n",
        "        # define loss function\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_one_hot))\n",
        "        # add the loss to the summary\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        \n",
        "        # add an optimiser\n",
        "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
        "        \n",
        "        # get the accuracy and log\n",
        "        self.accuracy = self._compute_accuracy(logits, y_one_hot)\n",
        "        tf.summary.scalar('acc', self.accuracy)\n",
        "        \n",
        "        # collect the summaries\n",
        "        self.merged = tf.summary.merge_all()\n",
        "        \n",
        "        # setup the initialisation operator\n",
        "        self.init_op = tf.global_variables_initializer()\n",
        "        \n",
        "    def _create_new_conv_layer(self, input, input_channels, output_channels, window_size,\n",
        "                               pool_size, filt_stride, pool_stride, name):\n",
        "        initializer = tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=False)\n",
        "        with tf.name_scope(name):\n",
        "            # setup the filter input shape for tf.nn.conv_2d\n",
        "            conv_filt_shape = [window_size[0], window_size[1], input_channels, output_channels]\n",
        "            # initialise weights and bias for the filter\n",
        "            weights = tf.Variable(initializer(conv_filt_shape), name='W')\n",
        "            bias = tf.Variable(tf.zeros((output_channels)), name='b')\n",
        "            # setup the convolutional layer operation\n",
        "            filt_stride = [1, filt_stride[0], filt_stride[1], 1]\n",
        "            out_layer = tf.nn.conv2d(input, weights, filt_stride, padding='SAME')\n",
        "            # add the bias\n",
        "            out_layer += bias\n",
        "            # apply a ReLU non-linear activation\n",
        "            out_layer = tf.nn.relu(out_layer)\n",
        "            pool_shape = [1, pool_size[0], pool_size[1], 1]\n",
        "            pool_strides = [1, pool_stride[0], pool_stride[1], 1]\n",
        "            out_layer = tf.nn.max_pool(out_layer, ksize=pool_shape, strides=pool_strides, padding='SAME')\n",
        "        return out_layer\n",
        "    \n",
        "    def _compute_accuracy(self, logits, labels):\n",
        "        prediction = tf.argmax(logits, 1)\n",
        "        equality = tf.equal(prediction, tf.argmax(labels, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "        return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pstp4UgDlWcX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, batch_size, epochs):\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(model.init_op)\n",
        "        train_writer = tf.summary.FileWriter(base_path +\n",
        "                                             \"{}/\".format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
        "                                             sess.graph)\n",
        "        for i in range(epochs):\n",
        "            image_batch, label_batch = get_batch(x_train, y_train, batch_size)\n",
        "            loss, _, acc = sess.run([model.loss, model.optimizer, model.accuracy],\n",
        "                                    feed_dict={model.input_images: image_batch.reshape(-1, 28, 28, 1),\n",
        "                                               model.labels: label_batch.reshape(-1, 1),\n",
        "                                               model.prob: 0.5})\n",
        "            if i % 50 == 0:\n",
        "                print(\"Iteration {} of {} - loss: {:.3f}, training accuracy: {:.2f}%\".\n",
        "                      format(i, epochs, loss, acc*100))\n",
        "                summary = sess.run(model.merged, feed_dict={model.input_images: image_batch.reshape(-1, 28, 28, 1),\n",
        "                                                    model.labels: label_batch.reshape(-1, 1)})\n",
        "                train_writer.add_summary(summary, i)\n",
        "        acc = sess.run(model.accuracy, feed_dict={model.input_images: x_test.reshape(-1, 28, 28, 1),\n",
        "                                                    model.labels: y_test.reshape(-1, 1)})\n",
        "        print(\"Final test accuracy is {:.2f}%\".format(acc * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhQmbnPGlWca",
        "colab_type": "code",
        "colab": {},
        "outputId": "9264d2a6-f509-4fb1-d695-1306395b6d48"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = ConvNet()\n",
        "    train_model(model, 32, 5000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 of 5000 - loss: 2.268, training accuracy: 15.62%\n",
            "Iteration 50 of 5000 - loss: 0.667, training accuracy: 78.12%\n",
            "Iteration 100 of 5000 - loss: 0.499, training accuracy: 81.25%\n",
            "Iteration 150 of 5000 - loss: 0.134, training accuracy: 96.88%\n",
            "Iteration 200 of 5000 - loss: 0.277, training accuracy: 87.50%\n",
            "Iteration 250 of 5000 - loss: 0.102, training accuracy: 96.88%\n",
            "Iteration 300 of 5000 - loss: 0.099, training accuracy: 96.88%\n",
            "Iteration 350 of 5000 - loss: 0.090, training accuracy: 96.88%\n",
            "Iteration 400 of 5000 - loss: 0.415, training accuracy: 93.75%\n",
            "Iteration 450 of 5000 - loss: 0.330, training accuracy: 87.50%\n",
            "Iteration 500 of 5000 - loss: 0.204, training accuracy: 93.75%\n",
            "Iteration 550 of 5000 - loss: 0.048, training accuracy: 100.00%\n",
            "Iteration 600 of 5000 - loss: 0.004, training accuracy: 100.00%\n",
            "Iteration 650 of 5000 - loss: 0.115, training accuracy: 93.75%\n",
            "Iteration 700 of 5000 - loss: 0.163, training accuracy: 93.75%\n",
            "Iteration 750 of 5000 - loss: 0.183, training accuracy: 93.75%\n",
            "Iteration 800 of 5000 - loss: 0.012, training accuracy: 100.00%\n",
            "Iteration 850 of 5000 - loss: 0.091, training accuracy: 93.75%\n",
            "Iteration 900 of 5000 - loss: 0.007, training accuracy: 100.00%\n",
            "Iteration 950 of 5000 - loss: 0.019, training accuracy: 100.00%\n",
            "Iteration 1000 of 5000 - loss: 0.003, training accuracy: 100.00%\n",
            "Iteration 1050 of 5000 - loss: 0.012, training accuracy: 100.00%\n",
            "Iteration 1100 of 5000 - loss: 0.227, training accuracy: 96.88%\n",
            "Iteration 1150 of 5000 - loss: 0.043, training accuracy: 100.00%\n",
            "Iteration 1200 of 5000 - loss: 0.046, training accuracy: 96.88%\n",
            "Iteration 1250 of 5000 - loss: 0.045, training accuracy: 100.00%\n",
            "Iteration 1300 of 5000 - loss: 0.051, training accuracy: 96.88%\n",
            "Iteration 1350 of 5000 - loss: 0.064, training accuracy: 96.88%\n",
            "Iteration 1400 of 5000 - loss: 0.011, training accuracy: 100.00%\n",
            "Iteration 1450 of 5000 - loss: 0.057, training accuracy: 96.88%\n",
            "Iteration 1500 of 5000 - loss: 0.106, training accuracy: 96.88%\n",
            "Iteration 1550 of 5000 - loss: 0.003, training accuracy: 100.00%\n",
            "Iteration 1600 of 5000 - loss: 0.013, training accuracy: 100.00%\n",
            "Iteration 1650 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 1700 of 5000 - loss: 0.032, training accuracy: 100.00%\n",
            "Iteration 1750 of 5000 - loss: 0.150, training accuracy: 96.88%\n",
            "Iteration 1800 of 5000 - loss: 0.067, training accuracy: 96.88%\n",
            "Iteration 1850 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 1900 of 5000 - loss: 0.079, training accuracy: 93.75%\n",
            "Iteration 1950 of 5000 - loss: 0.092, training accuracy: 96.88%\n",
            "Iteration 2000 of 5000 - loss: 0.006, training accuracy: 100.00%\n",
            "Iteration 2050 of 5000 - loss: 0.030, training accuracy: 100.00%\n",
            "Iteration 2100 of 5000 - loss: 0.323, training accuracy: 90.62%\n",
            "Iteration 2150 of 5000 - loss: 0.035, training accuracy: 100.00%\n",
            "Iteration 2200 of 5000 - loss: 0.009, training accuracy: 100.00%\n",
            "Iteration 2250 of 5000 - loss: 0.025, training accuracy: 100.00%\n",
            "Iteration 2300 of 5000 - loss: 0.025, training accuracy: 100.00%\n",
            "Iteration 2350 of 5000 - loss: 0.006, training accuracy: 100.00%\n",
            "Iteration 2400 of 5000 - loss: 0.133, training accuracy: 96.88%\n",
            "Iteration 2450 of 5000 - loss: 0.183, training accuracy: 93.75%\n",
            "Iteration 2500 of 5000 - loss: 0.067, training accuracy: 96.88%\n",
            "Iteration 2550 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 2600 of 5000 - loss: 0.059, training accuracy: 96.88%\n",
            "Iteration 2650 of 5000 - loss: 0.005, training accuracy: 100.00%\n",
            "Iteration 2700 of 5000 - loss: 0.013, training accuracy: 100.00%\n",
            "Iteration 2750 of 5000 - loss: 0.028, training accuracy: 100.00%\n",
            "Iteration 2800 of 5000 - loss: 0.043, training accuracy: 100.00%\n",
            "Iteration 2850 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 2900 of 5000 - loss: 0.064, training accuracy: 96.88%\n",
            "Iteration 2950 of 5000 - loss: 0.018, training accuracy: 100.00%\n",
            "Iteration 3000 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 3050 of 5000 - loss: 0.041, training accuracy: 96.88%\n",
            "Iteration 3100 of 5000 - loss: 0.015, training accuracy: 100.00%\n",
            "Iteration 3150 of 5000 - loss: 0.049, training accuracy: 96.88%\n",
            "Iteration 3200 of 5000 - loss: 0.078, training accuracy: 96.88%\n",
            "Iteration 3250 of 5000 - loss: 0.021, training accuracy: 100.00%\n",
            "Iteration 3300 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 3350 of 5000 - loss: 0.003, training accuracy: 100.00%\n",
            "Iteration 3400 of 5000 - loss: 0.034, training accuracy: 96.88%\n",
            "Iteration 3450 of 5000 - loss: 0.015, training accuracy: 100.00%\n",
            "Iteration 3500 of 5000 - loss: 0.173, training accuracy: 96.88%\n",
            "Iteration 3550 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 3600 of 5000 - loss: 0.126, training accuracy: 96.88%\n",
            "Iteration 3650 of 5000 - loss: 0.001, training accuracy: 100.00%\n",
            "Iteration 3700 of 5000 - loss: 0.004, training accuracy: 100.00%\n",
            "Iteration 3750 of 5000 - loss: 0.036, training accuracy: 96.88%\n",
            "Iteration 3800 of 5000 - loss: 0.008, training accuracy: 100.00%\n",
            "Iteration 3850 of 5000 - loss: 0.001, training accuracy: 100.00%\n",
            "Iteration 3900 of 5000 - loss: 0.020, training accuracy: 100.00%\n",
            "Iteration 3950 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 4000 of 5000 - loss: 0.091, training accuracy: 96.88%\n",
            "Iteration 4050 of 5000 - loss: 0.007, training accuracy: 100.00%\n",
            "Iteration 4100 of 5000 - loss: 0.005, training accuracy: 100.00%\n",
            "Iteration 4150 of 5000 - loss: 0.029, training accuracy: 96.88%\n",
            "Iteration 4200 of 5000 - loss: 0.010, training accuracy: 100.00%\n",
            "Iteration 4250 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 4300 of 5000 - loss: 0.001, training accuracy: 100.00%\n",
            "Iteration 4350 of 5000 - loss: 0.065, training accuracy: 96.88%\n",
            "Iteration 4400 of 5000 - loss: 0.006, training accuracy: 100.00%\n",
            "Iteration 4450 of 5000 - loss: 0.097, training accuracy: 96.88%\n",
            "Iteration 4500 of 5000 - loss: 0.002, training accuracy: 100.00%\n",
            "Iteration 4550 of 5000 - loss: 0.003, training accuracy: 100.00%\n",
            "Iteration 4600 of 5000 - loss: 0.118, training accuracy: 96.88%\n",
            "Iteration 4650 of 5000 - loss: 0.043, training accuracy: 100.00%\n",
            "Iteration 4700 of 5000 - loss: 0.031, training accuracy: 100.00%\n",
            "Iteration 4750 of 5000 - loss: 0.001, training accuracy: 100.00%\n",
            "Iteration 4800 of 5000 - loss: 0.007, training accuracy: 100.00%\n",
            "Iteration 4850 of 5000 - loss: 0.037, training accuracy: 96.88%\n",
            "Iteration 4900 of 5000 - loss: 0.117, training accuracy: 96.88%\n",
            "Iteration 4950 of 5000 - loss: 0.001, training accuracy: 100.00%\n",
            "Final test accuracy is 99.08%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}